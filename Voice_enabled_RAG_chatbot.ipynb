{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\.conda\\envs\\MCHATBOT\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = Path('.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "PINECONE_API_KEY=os.getenv('PINECONE_API_KEY')\n",
    "os.environ['PINECONE_API_KEY']=PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract data from the pdf\n",
    "\n",
    "def load_pdf(data):\n",
    "    loader=DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    documents=loader.load() \n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='scores are uniformly distributed, we might then suppose that their average is\\nalso uniformly distributed. \\nBut is this actually true? \\nBegin by considering the\\nextremes: there is only one way to obtain a mean test score of 300; both\\nindividuals must score 300. \\nSimilarly, to obtain a mean of 0, both individuals\\nmust score 0. \\nBy contrast, consider a mean of 150. \\nThis could result from a\\nnumber of individual score combinations, for example (\\nscore\\nA\\n, \\nscore\\nB\\n) = :\\n(150,150),(100,200),(125,175). \\nIntuitively, there are many more ways to\\nobtain moderate values for the sample mean than there are for the extremes.\\nThis central tendency of the sample mean increases along with sample size,\\nsince extreme values then require more individual scores to be\\nsimultaneously extreme, which is less likely. \\nThis effect is visible in \\nFigure\\n3.15\\n; however, we also see another impact on the probability distribution for\\nthe mean: as our sample size increases, the distribution is an increasingly\\ngood fit to the normal distribution. \\nThis approximation, it turns out, becomes\\nexact in the limit of an infinite sample size and is known as the \\ncentral limit\\ntheorem (CLT)\\n. \\nFor practical purposes, however, the approximation is\\ngenerally reasonable if the sample size is above about 20 (see right-hand\\npanel of \\nFigure 3.15\\n).\\nThere are, in fact, a number of central limit theorems. \\nThe above CLT applies\\nto the average of independent, identically distributed random variables.\\nHowever, there are also central limit theorems that apply far less stringent\\nconditions. \\nThis means that whenever an output is the result of the sum or\\naverage of a number of largely independent factors, then it may be reasonable\\nto assume it is normally distributed. \\nFor example, one can argue that an\\nindividual’s intelligence is the result of the average of a number of factors,\\nincluding parenting, genetics, life experience and health, among others.\\nHence, we might assume that an individual’s test score picked at random\\nfrom the population is normally distributed.\\nAn introduction to central limit theorems\\nFigure 3.15\\n The central limit theorem in action: as we increase the sample\\nsize (left to right), the probability distribution for the sample mean (red lines)\\napproaches a normal with the same mean and standard deviation (black\\nlines).', metadata={'source': 'data\\\\A students guide to bayesian statistics - Ben Lambert.pdf', 'page': 100})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data[100] #sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20) ## overlapp b/w embedddings\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of chunks:  2443\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"length of chunks: \",len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hf_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=download_hf_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result=embeddings.embed_query(\"Hello World\")\n",
    "print(\"Length\",len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import time\n",
    "import playsound\n",
    "import random \n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    ls=[i for i in range(1,100)]\n",
    "    r1=random.choice(ls)\n",
    "    r2=random.choice(ls)\n",
    "    filename = 'voice_'+str(r1)+'_'+str(r2)+'.mp3'\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "\n",
    "#speak(\"Hi Mayank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio():\n",
    "\tr = sr.Recognizer()\n",
    "\twith sr.Microphone() as source:\n",
    "\t\taudio = r.listen(source)\n",
    "\t\tsaid = \"\"\n",
    "\n",
    "\t\ttry:\n",
    "\t\t    said = r.recognize_google(audio)\n",
    "\t\t    print(\"Query: \"+said)\n",
    "\t\texcept Exception as e:\n",
    "\t\t    print(\"Exception: \" + str(e))\n",
    "\n",
    "\treturn said\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi's capital of India\n",
      "Delhi's capital of India\n"
     ]
    }
   ],
   "source": [
    "text_gen=get_audio()\n",
    "print(text_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"mchatbot\"\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing a given pinecone index/knowledge base\n",
    "index_name=\"mchatbot\"\n",
    "#docsearch=PineconeVectorStore.from_documents(text_chunks, embeddings, index_name=index_name)\n",
    "docs_chunks =[t.page_content for t in text_chunks]\n",
    "docsearch=PineconeVectorStore.from_texts(docs_chunks ,embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: explain posterior in detail\n",
      "the posterior distribution:\n",
      "The posterior is the synthesis of past experience and information from\n",
      "observed data and represents our updated state of knowledge. \n",
      "The uncertainty\n",
      "in the posterior is usually (although not always) reduced compared to the\n",
      "prior because the data allows us to better understand the world.\n"
     ]
    }
   ],
   "source": [
    "#query = \"What is Bayesian inference?\"\n",
    "query=get_audio()\n",
    "docs = vectorstore.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\" \n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just state that you don't know, don't try to make up an answer.\n",
    "\n",
    "\n",
    "Context:{context}\n",
    "Question:{question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "\n",
    "Helpful answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])\n",
    "chain_type_kwargs={\"prompt\":PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"model/llama-7b.ggmlv3.q4_1.bin\",\n",
    "                 model_type=\"llama\",\n",
    "                 config={'max_new_tokens':256,\n",
    "                        'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k':1}),\n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is central limit theorem\n",
      "====================================================================================================\n",
      "Response :  The Central Limit Theorem states that, if we have enough observations in a sample, then the sampling distribution of the mean will look approximately normal. \n",
      "This statement implies that an increase in size of the sample increases the probability that the sampling distribution of the mean will be normally distributed. \n",
      "Based on this fact and Figure 3.15 (which shows the histogram for\n",
      "an unweighted random sample with 20 observations), we can say that the sampling\n",
      "distribution will probably not have a normal distribution if the size of the\n",
      "sample is less than or equal to about 20. \n",
      "\n",
      "\n",
      "\"\"\"\n",
      "from numpy import linspace, randint, array, pi, cos, sin, pi / 4, pi * sqrt(3)\n",
      "from scipy.stats import gamma\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "def get_answer():\n",
      "    \"\"\"Return the answer.\"\"\"\n",
      "    return \"The Central Limit Theorem states that, if we have enough observations in a sample, then the sampling distribution of the mean will look approximately normal.  This statement implies that an increase in size of the sample increases the probability that the sampling distribution of the mean will be normally distributed. Based on this\n",
      "Query: exit\n",
      "====================================================================================================\n",
      "Shutting down RAG\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    #user_input=input(f\"Input Prompt:\")\n",
    "    user_input=get_audio()\n",
    "    print(\"\".join([\"=\"]*100))\n",
    "    if(user_input=='exit' or user_input=='Exit'):\n",
    "        print('Shutting down RAG')\n",
    "        break\n",
    "    result=qa({\"query\":user_input})\n",
    "    print(\"Response : \",result[\"result\"])\n",
    "    speak(result[\"result\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "mchatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
