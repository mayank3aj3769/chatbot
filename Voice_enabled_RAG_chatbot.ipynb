{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\.conda\\envs\\MCHATBOT\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = Path('.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "PINECONE_API_KEY=os.getenv('PINECONE_API_KEY')\n",
    "os.environ['PINECONE_API_KEY']=PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract data from the pdf\n",
    "\n",
    "def load_pdf(data):\n",
    "    loader=DirectoryLoader(data,\n",
    "                    glob=\"*.pdf\",\n",
    "                    loader_cls=PyPDFLoader)\n",
    "    documents=loader.load() \n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data) #sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='scores are uniformly distributed, we might then suppose that their average is\\nalso uniformly distributed. \\nBut is this actually true? \\nBegin by considering the\\nextremes: there is only one way to obtain a mean test score of 300; both\\nindividuals must score 300. \\nSimilarly, to obtain a mean of 0, both individuals\\nmust score 0. \\nBy contrast, consider a mean of 150. \\nThis could result from a\\nnumber of individual score combinations, for example (\\nscore\\nA\\n, \\nscore\\nB\\n) = :\\n(150,150),(100,200),(125,175). \\nIntuitively, there are many more ways to\\nobtain moderate values for the sample mean than there are for the extremes.\\nThis central tendency of the sample mean increases along with sample size,\\nsince extreme values then require more individual scores to be\\nsimultaneously extreme, which is less likely. \\nThis effect is visible in \\nFigure\\n3.15\\n; however, we also see another impact on the probability distribution for\\nthe mean: as our sample size increases, the distribution is an increasingly\\ngood fit to the normal distribution. \\nThis approximation, it turns out, becomes\\nexact in the limit of an infinite sample size and is known as the \\ncentral limit\\ntheorem (CLT)\\n. \\nFor practical purposes, however, the approximation is\\ngenerally reasonable if the sample size is above about 20 (see right-hand\\npanel of \\nFigure 3.15\\n).\\nThere are, in fact, a number of central limit theorems. \\nThe above CLT applies\\nto the average of independent, identically distributed random variables.\\nHowever, there are also central limit theorems that apply far less stringent\\nconditions. \\nThis means that whenever an output is the result of the sum or\\naverage of a number of largely independent factors, then it may be reasonable\\nto assume it is normally distributed. \\nFor example, one can argue that an\\nindividual’s intelligence is the result of the average of a number of factors,\\nincluding parenting, genetics, life experience and health, among others.\\nHence, we might assume that an individual’s test score picked at random\\nfrom the population is normally distributed.\\nAn introduction to central limit theorems\\nFigure 3.15\\n The central limit theorem in action: as we increase the sample\\nsize (left to right), the probability distribution for the sample mean (red lines)\\napproaches a normal with the same mean and standard deviation (black\\nlines).', metadata={'source': 'data\\\\A students guide to bayesian statistics - Ben Lambert.pdf', 'page': 100})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20) ## overlapp b/w embedddings\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of chunks:  2625\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"length of chunks: \",len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model , and sentence \n",
    "def download_hf_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=download_hf_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result=embeddings.embed_query(\"Hello World\")\n",
    "print(\"Length\",len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import time\n",
    "import playsound\n",
    "import random \n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    ls=[i for i in range(1,100)]\n",
    "    r1=random.choice(ls)\n",
    "    r2=random.choice(ls)\n",
    "    filename = 'voice_'+str(r1)+'_'+str(r2)+'.mp3'\n",
    "    tts.save(filename)\n",
    "    playsound.playsound(filename)\n",
    "\n",
    "#speak(\"Hi Mayank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio():\n",
    "\tr = sr.Recognizer()\n",
    "\twith sr.Microphone() as source:\n",
    "\t\taudio = r.listen(source)\n",
    "\t\tsaid = \"\"\n",
    "\n",
    "\t\ttry:\n",
    "\t\t    said = r.recognize_google(audio)\n",
    "\t\t    print(\"Query: \"+said)\n",
    "\t\texcept Exception as e:\n",
    "\t\t    print(\"Exception: \" + str(e))\n",
    "\n",
    "\treturn said\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"mchatbot\"\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing a given pinecone index/knowledge base\n",
    "index_name=\"mchatbot\"\n",
    "#docsearch=PineconeVectorStore.from_documents(text_chunks, embeddings, index_name=index_name)\n",
    "docs_chunks =[t.page_content for t in text_chunks]\n",
    "docsearch=PineconeVectorStore.from_texts(docs_chunks ,embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: for highly precious droplets how much charge is lost by the parent droplet during rally fission\n",
      "is observed, accompanied by ion emission which is however insufﬁcient to prevent theCoulomb explosion. Ion emission and the smaller progeny droplets account for 24 % and\n",
      "16 % of the initial charge\n",
      ", respectively.\n",
      "Key words: breakup/coalescence, electrohydrodynamic effects\n",
      "†Email address for correspondence: mgameroc@uci.edu\n",
      "© The Author(s), 2023. Published by Cambridge University Press. This is an Open Access article,\n"
     ]
    }
   ],
   "source": [
    "#query = \"What is Bayesian inference?\"\n",
    "query=get_audio()\n",
    "#query=\"For highly viscous droplets, how much charge is lost by the parent droplet during Rayleigh fission?\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\" \n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just state that you don't know, don't try to make up an answer.\n",
    "\n",
    "\n",
    "Context:{context}\n",
    "Question:{question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "\n",
    "Helpful answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])\n",
    "chain_type_kwargs={\"prompt\":PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"model/llama-7b.ggmlv3.q4_1.bin\",\n",
    "                 model_type=\"llama\",\n",
    "                 config={'max_new_tokens':512,\n",
    "                        'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k':2}),\n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Response :  \n",
      "\n",
      "Ionic liquids with very large radii (e.g., C6H5O5-CH3OH) are known to be unstable at low ow rates, because they can undergo Rayleigh fission when a droplet is in ight.\n",
      "This instability is well described by ρV(<) where V is the ow rate of liquid and R is the radius\n",
      "of ionic liquid. \n",
      "\n",
      "In the ionic liquids EMI-Im,MIs that do not I-Im, MI/F2A-Im (CMI-Im-Im-Im-Im-Im(NOIM-10403-Im and C6HMI-Im, M IMI-IMiMe-Im-im or -Im–TGIL, N , R \n",
      "mi-MI-MIm and Im, which have notably to J MI-O/O21405OH-Im-Im-Im, MI, HIM-EMI-IM (EMIm-Im-Im, M IMI-Im, the ionic-Im, M IMI-Im and EMI-MII-MI-Im, M IMI-AL (HIOA (em Im and F 09 and CH3O33S-Im-Im and BMlEMI-EMiMIm and TMSI and DMS-Im, M IMI-Im, the ion and C6HIM-Im+27HT-Im,MI-MI-102-MI-C-Im, M IMI-Im, M IMI-L403-Im(im− and F MI-EM-Im-MI-MI-Im (CH-N3MII-Im, the radius is Im-Im–F3 [EI-Im with very small radii that under test electrospontheIM-ImiO7026HMDLMI-Im and EMI-Im-Im, M I-MI-MI+NKMB-Im (CMI-Im and Im, F EMI-Im,MII-Im, the Im-131111\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "Response :  \n",
      "1) Field emission of charged droplet reduces droplet size and increases ion current. In general we expect that the ion current is increased because the charge on a given liquid particle decreases with distance from its surface, so that there are more ion particles for every charge. For example, if a droplet has radius $r$ and emits ions at a rate $i$\n",
      "and it charges to a potential of $E_C$, then $d\\varphi$ ﬁgures out the number of ion particles per charge\n",
      "from this distance: $$ i = \\frac{q}{4\\pi E_{π( \\pi r^pi R_{\\pi r}\\pi r^rmeVEC} \\pi \\pi r\\pi\\pi\\pi\\pi\\pi r\\pi rcnp  \\pi\\pi\\pi\\pi\\pi\\pi\\pi \\pi \\pi r\\pi r\\pi r\\pi k_kBeta r^oerfc\\pi r\\pi r E_{pi{e}{\\rm{rhos}ECE _E[Pi\\pi R \\pi r}Airho r}\\left(\\pi r\\pi r\\pi \\pi \\pi r\\pi r }Rmspace\\pi\\pi \\pi \\pi \\pi \\pi\\pi\\pi r^3pi\\pi \\pi \\pi r}r_pi \\pi \\pi\\pi\\pi\\pi\\pi\\pi\\pi\n",
      " \\pi\\pi\\pi \\pi \\pi r}e^{o\\pi\\pi r} π \\pi\\pi\\pi\\pi \\pi \\pi r}\\left(\\pi\\,\n",
      "Pi r^rpi{}aPl*{pi\\pi\\pi r}E_pi\\pi r E_pirho r \\pi \\pi \\pi \\pi \\pi r\\pi(r_pi\\pi r_π \\pi r\\pi r} π r^oerfc\\pi \\pi \\pi \\pi\\pi\\pi \\pi \\pi r\\pi r^pi \\pi \\pi r}Epsilon}\\left(\\pi r\\pi e c ln}{\\pi \\pi \\pi \\pi \\pi \\pi \\pi \\pi \\pi\\pi\\pi\n",
      "Pi (4\\pi \\pi r}r_pir{ π d_{pi r\\pi r^pi\\pi\\pi\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    #user_input=input(f\"Input Prompt:\")\n",
    "    user_input=get_audio()\n",
    "    print(\"\".join([\"*\"]*100))\n",
    "    if(user_input=='exit' or user_input=='Exit'):\n",
    "        print('Shutting down RAG')\n",
    "        break\n",
    "    result=qa({\"query\":user_input})\n",
    "    print(\"Response : \",result[\"result\"])\n",
    "    print(\"\".join([\"*\"]*100))\n",
    "    speak(result[\"result\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "mchatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
